---
title: CUDA架构
date: 2023-04-16 16:00:00
tags:
- cuda
categories:
- 学习
---

## 疑问

- ampere架构SM中有4个process block，process block对应一个warp？意思是可以有4个warp同时执行？
  - Femi架构没有process block，SM就是最小单元？
- The threads of a thread block execute concurrently on one multiprocessor, and multiple thread blocks can execute concurrently on one multiprocessor.
  - 这样岂不是若干thread block抢一个SM上的shared memory？
  - 不同threadblock的warp并发执行，如何隐藏延迟

- cuda分块大小对性能影响很大，那么如何确定分块大小呢？
  - 穷举
  - 分析模型？

<!-- more -->

# NV架构列表

## compute capacity

[Matching CUDA arch and CUDA gencode for various NVIDIA architectures - Arnon Shimoni](https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/)

- Pascal
  - SM61: 1080
- volta
  - SM70: Tesla V100, Titan V
- Ampere
  - SM80: NVIDIA A100 (the name “Tesla” has been dropped – GA100))
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20230419195715.png)


- Tesla(2008)
- Fermi(2010)
- Kepler(2012)：**K80**
- Maxwell(2014)： M10/M40
- Pascal(2016)： Tesla **P40**、**P100**、GTX **1080Ti**  Titan XP、Quadro GP100/P6000/P5000，10系
- Volta(2017)： Tesla **V100**、GeForce **Titan V**、Quadro GV100专业卡
- Turing(2018)： 1个SM 8个Tensor core，1个RT core，16，20系
- Ampere(2020)： **A100**，30系
- Hopper(2022)：H100

1080: 20x128
1080ti: 28x128, gp104
p40: 30x128, gp102
p100: 28x128, HBM2(4096bit)

[Ampere (microarchitecture) - Wikipedia](https://en.wikipedia.org/wiki/Ampere_(microarchitecture))

![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/picgo/20230404165003.png)

[NVIDIA GPU 架构梳理 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/394352476)

### Fermi

-   橙色部分：2 个 Warp Scheduler/Dispatch Unit
-   绿色部分：32 个 CUDA 内核，分在两条 lane 上，每条分别是 16 个

https://stackoverflow.com/a/10467342
> The programmer divides work into threads, threads into thread blocks, and thread blocks into grids. The compute work distributor allocates thread blocks to Streaming Multiprocessors (SMs). **Once a thread block is distributed to a SM the resources for the thread block are allocated** (warps and shared memory) and threads are divided into groups of 32 threads called warps. Once a warp is allocated it is called an active warp. **The two warp schedulers pick two active warps per cycle** and dispatch warps to execution units. For more details on execution units and instruction dispatch see [1](http://www.nvidia.com/content/PDF/fermi_white_papers/NVIDIA_Fermi_Compute_Architecture_Whitepaper.pdf) p.7-10 and [2](http://www.anandtech.com/show/3809/nvidias-geforce-gtx-460-the-200-king/2).

Fermi，一个SM有两个warp保证每周期有指令可以发射
> A stalled warp is ineligible to be selected by the warp scheduler. On Fermi it is useful to have at least 2 eligible warps per cycle so that the warp scheduler can issue an instruction.

GeForce 560Ti，8SM，每个48CUDA
>  If you launch kernel<<<8, 48>>> you will get 8 blocks each with 2 warps of 32 and 16 threads. There is no guarantee that these 8 blocks will be assigned to different SMs.

- 每个SM可以有很多线程块
> A GTX560 can have 8 SM * 8 blocks = 64 blocks at a time or 8 SM * 48 warps = 512 warps if the kernel does not max out registers or shared memory. At any given time on a portion of the work will be active on SMs. Each SM has multiple execution units (more than CUDA cores). Which resources are in use at any given time is dependent on the warp schedulers and instruction mix of the application. If you don't do TEX operations then the TEX units will be idle. If you don't do a special floating point operation the SUFU units will idle.

![](https://pic2.zhimg.com/80/v2-5aaf90a4f9cb41af90833a978d735c89_720w.webp)

#### 白皮书
[Microsoft Word - NVIDIA Fermi Architecture Whitepaper.docx](https://www.nvidia.com/content/PDF/fermi_white_papers/NVIDIA_Fermi_Compute_Architecture_Whitepaper.pdf)

双发射
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/picgo/20230414193634.png)

> Fermi’s dual warp scheduler selects two warps, and issues one instruction from each warp to a group of sixteen cores, sixteen load/store units, or four SFUs. Because warps execute independently, Fermi’s scheduler does not need to check for dependencies from within the instruction stream. Using this elegant model of dual-issue, Fermi achieves near peak hardware performance.
> Most instructions can be dual issued; two integer instructions, two floating instructions, or a mix of integer, floating point, load, store, and SFU instructions can be issued concurrently. Double precision instructions do not support dual dispatch with any other operation.

可配置的shared memory和L1 cache
> G80 and GT200 have 16 KB of shared memory per SM. In the Fermi architecture, each SM has 64 KB of on-chip memory that can be configured as 48 KB of Shared memory with 16 KB of L1 cache or as 16 KB of Shared memory with 48 KB of L1 cache.

### Kepler

- 一个SMX 192个CUDA
![](https://pic3.zhimg.com/80/v2-8130651bd394205a5f9fb9c736085b96_720w.webp)

### Maxwell

- SMM：四个处理块(processing block)，每个有专用的warp调度器，包含32个core
![](https://pic3.zhimg.com/80/v2-3cd6ea7b8bfd5830760e022393da0b1a_720w.webp)

### volta
跳过了pascal：一个SM两个处理块

- SM：4个block
- 将一个CUDA拆分成FP32和INT32，每个周期可以同时执行浮点和整数。
- 添加tensor core
![](https://pic4.zhimg.com/80/v2-ab5cc1ac8a897332cdb9d6565cf9c7af_720w.webp)

### ampere架构

跳过turing：去掉了F64

![](https://pic2.zhimg.com/80/v2-ab9a493303f4902b1dace22df0fb652d_720w.webp)

# CUDA架构

## 参考资料

- [CUDA C++ Programming Guide (nvidia.com)](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)

## 基础

In CUDA programming, both CPUs and GPUs are used for computing. Typically, we refer to CPU and GPU system as _host_ and _device_, respectively. CPUs and GPUs are separated platforms with their own memory space. Typically, we run serial workload on CPU and offload parallel computation to GPUs.

- 三个关键抽象：
  - 层次化的线程组
  - 共享内存
  - 同步

高度可扩展性：
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/picgo/20230404193556.png)

## 编程模型

### 线程层次

kernel
- C++函数，被调用时，会被每个CUDA线程并行执行。
- 使用__global__声明kernel函数
- 使用`<<<...>>>`_execution configuration_ syntax，指定使用多少线程执行该kernel

线程层次
- block：多个线程组成一个线程块。可以是1,2,3维的
  - 通过`threadIdx`索引。如二维 _(x, y)_ 的线程id是 _(x + y Dx)_;
  - 一个块内的线程数是有限制的（当前的GPU一般为1024个）。因为一个块内的线程会被调度到一个SM(streaming multiprocessor core)中，共享该SM的片上存储（shared memory）
  - 一个块是独立的，可以以任意顺序调度，从而保证了GPU的可扩展性（SM是基本单元，堆SM）
  - shared memory延迟很低，类似于L1 cache
- grid：多个线程块组成一个grid。可以是1,2,3维的
  - 通过`blockIdx`索引

以下代码声明了1个线程块，大小为NxN。用于将两个NxN的矩阵相加。
```c
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
                       float C[N][N])
{
    int i = threadIdx.x;
    int j = threadIdx.y;
    C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation with one block of N * N * 1 threads
    int numBlocks = 1;
    dim3 threadsPerBlock(N, N);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}
```

以下代码声明了N/16 x N/16个线程块，每个大小为16x16。用于将两个NxN的矩阵相加。
```c
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
float C[N][N])
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    if (i < N && j < N)
        C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}
```

![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/picgo/20230404195709.png)

- 在线程块内，线程通过shared memory来共享数据。并且通过同步操作来协调内存访问
  - `__syncthreads()`用于路障同步


- 线程块蔟：
  - CUDA 9.0中引入的一个可选层次
  - 类似于线程块内线程保证在同一个SM。一个cluster内的线程块被调度到同一个GPU Processing Cluster (GPC)
  - 大小一般最大8个块
  - 支持硬件支持的同步api。cluster.sync()
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/picgo/20230404200253.png)


Threads within a CTA execute in SIMT (single-instruction, multiple-thread) fashion in groups called _warps_.

SIMD和SIMT的区别：SIMT编程时可以控制单个线程。
>  A key difference is that SIMD vector organizations expose the SIMD width to the software, whereas SIMT instructions specify the execution and branching behavior of a single thread. In contrast with SIMD vector machines, SIMT enables programmers to write thread-level parallel code for independent, scalar threads, as well as data-parallel code for coordinated threads.

SM能一次能执行多少block，和每线程register数和block使用的shared memory有关。因为SM的寄存器和shared memory是给batch block所有线程间分配的。如果一个块都执行不了，则kernel无法启动。


### 编程接口

- 包含对C++的少量扩展和rutime库

C++扩展
- 定义kernel
- 指定线程数

CUDA runtime
- 执行在host上
- 分配回收device内存
- 在host和device间传输数据
- 管理多个device

编译
- 将device代码编译成ptx或cubin
- 将host代码编译，和runtime链接
  - runtime基于底层另一层抽象层，该抽象层再基于driver API。

兼容性
- cubin只能在小版本里后向兼容。_cubin_ object generated for compute capability _X.y_ will only execute on devices of compute capability _X.z_ where _z≥y_.
- PTX可以后向兼容，但是无法利用新硬件特性。a binary targeting devices of compute capability 7.0 (Volta) compiled from PTX generated for compute capability 6.0 (Pascal) will not make use of Tensor Core instructions, since these were not available on Pascal.
- 后向兼容(backward)：旧编译的可以在新平台上运行

![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/picgo/20230406141727.png)

## 硬件实现

https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation

> The NVIDIA GPU architecture is built around a scalable array of multithreaded _Streaming Multiprocessors_ (_SMs_). When a CUDA program on the host CPU invokes a kernel grid, the blocks of the grid are enumerated and distributed to multiprocessors with available execution capacity. The threads of a thread block execute concurrently on one multiprocessor, and multiple thread blocks can execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are launched on the vacated multiprocessors.

- 英伟达GPU架构由SM数组组成，具有可扩展性。
- 当host上的一个CUDA程序调用一个kernel grid时，grid中的线程块被分发到有计算能力的SM上执行。
- 一个线程块内的线程在一个multiprocessor内并发执行，并且多个线程块也可以并发调度到一个SM上（当一个线程块终止时，新的块补上） *p.s 这里说的是并发，可能需要和并行区分*

- SM被设计来并发执行上百个线程，采用了SIMT架构（_Single-Instruction, Multiple-Thread_）
  - 单线程内利用流水线实现ILP
  - 通过同时多线程（simultaneous hardware multithreading）实现**线程级并行**
    - 和CPU的SMT不同。Unlike CPU cores, they are issued in order and there is no branch prediction or speculative execution.

### SIMT

自己的理解：CPU一个核可以并发执行两个线程（超线程技术），而GPU一个SM可以并发运行成百上千的线程。为了做到这点，采用了SIMT技术

- warp
- warp内的线程执行

- SM以一个warp 32个线程为单位，进行调度。The multiprocessor creates, manages, schedules, and executes threads in groups of 32 parallel threads called _warps_. 
- 每个warp的线程从相同的PC开始执行。但是它们**内部有自己的PC可以单独跳转**。Individual threads composing a warp start together at the same program address, but they have their own instruction address counter and register state and are therefore free to branch and execute independently.
- **SM将线程块划分为warp进行调度**。When a multiprocessor is given one or more thread blocks to execute, it partitions them into warps and each warp gets scheduled by a _warp scheduler_ for execution
- 线程块的划分很简单，连续的线程被划分在一起。The way a block is partitioned into warps is always the same; each warp contains threads of consecutive, increasing thread IDs with the first warp containing thread 0.
- warp中的线程从相同地址开始执行，如果线程因为数据相关的分支造成分叉，**warp执行每一条代码路径**，同时禁止非该代码路径上的线程。A warp executes one common instruction at a time. ... If threads of a warp diverge via a data-dependent conditional branch, the warp executes each branch path taken, disabling threads that are not on that path.
- **分叉只发生在warp内**，不同warp是独立的。Branch divergence occurs only within a warp; different warps execute independently regardless of whether they are executing common or disjoint code paths.

- **GPU的SIMT和SIMD有点类似，都是单条指令处理多个数据**。The SIMT architecture is akin to SIMD (Single Instruction, Multiple Data) vector organizations in that a single instruction controls multiple processing elements.
  - 关键的不同在于，SIMT既可以实现线程级并行（对于独立的标量线程），又可以实现数据级并行（对于合作线程）。SIMT enables programmers to write thread-level parallel code for independent, scalar threads, as well as data-parallel code for coordinated threads.

- volta之前的架构，warp内32个线程公用相同的PC。导致分叉路径上的线程无法相互通信。Prior to NVIDIA Volta, warps used a single program counter shared amongst all 32 threads in the warp together with an active mask specifying the active threads of the warp. As a result, threads from the same warp in divergent regions or different states of execution cannot signal each other or exchange data, and algorithms requiring fine-grained sharing of data guarded by locks or mutexes can easily lead to deadlock, depending on which warp the contending threads come from.

### Hardware Multithreading

- 执行上下文包含PC，寄存器。warp上下文被保存在片上（而不是软件保存），因此warp切换没有损失。The execution context (program counters, registers, and so on) for each warp processed by a multiprocessor is maintained on-chip during the entire lifetime of the warp. Therefore, switching from one execution context to another has no cost, 

## Shared memory or cache ?

[Is it possible to use L1 cache instead of shared memory when implementing blocked matmuls in CUDA - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums](https://forums.developer.nvidia.com/t/is-it-possible-to-use-l1-cache-instead-of-shared-memory-when-implementing-blocked-matmuls-in-cuda/256985/3)

起初没有L1/L2 --> 引入scratch pad --> cache越来越大
> I think it is fair to say that the importance of shared memory in CUDA programming has decreased with the advent of L1/L2 caches of competitive size in GPUs. For use cases requiring peak performance, shared memory can still be important due to the programmer control it provides.


## tensor core

v100 whitepaper
> The Volta tensor cores are accessible and exposed as Warp-Level Matrix Operations in the CUDA 9 C++ API. The API exposes specialized matrix load, matrix multiply and accumulate, and matrix store operations to efficiently use Tensor Cores from a CUDA-C++ program. At the CUDA level, the warp-level interface assumes 16x16 size matrices spanning all 32 threads of the warp. In addition to CUDA-C++ interfaces to program Tensor Cores directly, cuBLAS and cuDNN libraries have been updated to provide new library interfaces to make use of Tensor Cores for deep learning applications and frameworks. NVIDIA has worked with many popular deep learning frameworks such as Caffe2 and MXNet to enable use of Tensor Cores for deep learning research on Volta GPU based systems. NVIDIA is working to add support for Tensor Cores in other frameworks as well.


# aaa

## unified memory

[Beyond GPU Memory Limits with Unified Memory on Pascal | NVIDIA Technical Blog](https://developer.nvidia.com/blog/beyond-gpu-memory-limits-unified-memory-pascal/)

背景
- 两个memory space
- 应用不能oversubscribing GPU内存，开发者必须手动管理active working set
- 双、四和八 GPU 系统在工作站和大型超级计算机中变得越来越普遍，在CPU 和GPU 之间手动管理数据很困难
- 某些应用程序基本无法手动管理：光线追踪引擎发射的光线可以根据材料表面向任何方向反弹。如果场景不适合 GPU 内存，光线可能很容易击中不可用的表面，必须从 CPU 内存中获取。在这种情况下，如果没有真正的 GPU 页面故障功能，几乎不可能计算出哪些页面应该在什么时间迁移到 GPU 内存。

特点
- 统一内存于 2014 年随 CUDA 6 和 Kepler 架构一起推出。这种相对较新的编程模型允许 GPU 应用程序在 CPU 函数和 GPU 内核中使用单个指针，从而大大简化了内存管理。
- CUDA 8 和 Pascal 架构通过添加 49 位虚拟寻址和按需页面迁移(**on-demand page migration**)，显著改进了统一内存功能。
    -  简单来说就是实现了page fault。The Page Migration engine allows GPU threads to fault on non-resident memory accesses so the system can migrate pages from anywhere in the system to the GPUs memory on-demand for efficient processing.
- CUDA 8 还添加了通过向运行时提供提示来优化数据局部性的新方法，因此仍然可以完全控制数据迁移。

## memory space

local
global 
co

# CUDA and Applications to Task-based Programming
[CUDA and Applications to Task-based Programming (cuda-tutorial.github.io)](https://cuda-tutorial.github.io/)

## part1 编程模型

- CPU latency-oritened
    - large L1
    - ILP
- GPU througput-oriented
    - vast number of parallel processors
    - over-subscribe, latency hiding
- CUDA: Compute Unified Device Architecture
    - driver API: cu
    - runtime API: cuda
    - device runtime API
    - driver API is superset of runtime API, provide a few additional advanced features.

### 有用的参考资料
- Essential reading
    - CUDA Programming Guide 
    - CUDA API Reference Manual 
    - PTX Instruction Set Architecture 
- Building executables
    - CUDA Compiler Driver NVCC 
- Debugging & profiling
    - CUDA-MEMCHECK
    - Nsight Documentatio

`__global__`: be invoked straight from the host and must not have a return value other than void.
- Launch configuration, parameters (built-in types, structs, pointers)
`__device__`: be called from functions already running on the device, such as kernels or other device functions.
`__host__`: 修饰运行在CPU上的函数
- 同时指定device和host，可以用于实现一些架构无关的函数

同步
- kernel call对于host是异步的，然而kernel call之间默认不是异步的，因此cuda默认假设连续的kernel calls or copy instructions are dependent on previous events, and order them accordingly。
- 同步命令
    - cudaDeviceSynchronize() to synchronize CPU and GPU
    - cudaEventSynchronize() to synchronize up to certain event

### 运行

使用warp的原因，为了利用SIMD单元
For the sake of exploiting SIMD hardware units, threads will always execute in groups of 32, regardless of the block size being used.

volta改进了warp（单独PC）
Before Volta: not understanding warps may crash your application 
After Volta: not caring about warps may make your application slower

SM
- **cuda core**: synonym for the units that perform **integer or floating-point arithmetic**
- LD/ST
- SFU
- tensor core




### warp执行模型

**block queue**
- the blocks that make up a grid are committed to the GPU in a block queue.
- GPU will then proceed to process the blocks in parallel. The degree of parallelisms depends on the hardware being used but is transparent to the developer:
block被完全分配给一个SM
**一个SM可以运行多个block**
- **如何考虑shared memory的共享？**
SM选择ready的warp执行，
- SM的warp越多，并行效率越高？The more warps an SM has to choose from, the higher the chances are that it can hide latency by switching to different warps.

warp作为一个整体执行，warp中的线程同时执行下一条指令
- 线程可以分叉（diverged）

![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231218194445.png)


SIMT正式解释：既可以看作单独的线程，又可以像SIMD一样高效（只要不分叉）
- 每个线程有一个active flag控制是否参与warp内的计算
This architecture design, which enables threads to behave like individual entities, while still enabling the exploitation of efficient SIMD operations when threads are not diverged is described by the term “same-instruction-multiple-threads”, or SIMT for short.

### CUDA thread execution model

- Legacy Thread Scheduling
- Independent Thread Scheduling (ITS)

legacy(“lockstep”)
- **warp只有一个PC值**
    - warp 中的所有线程在每个时钟周期都执行相同的指令
- inactive will not execute current instruction
- diverges时，先执行一部分，再执行另一部分
- 在warp切换前，会尝试达到合并点。Diverged threads will try to reach convergence point before switching
- 位于分叉的两部分的线程，不能实现一些同步算法，容易造成死锁
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231218202552.png)


ITS(Independent Thread Scheduling)
- Two registers reserved, each thread gets its own program counter
- 线程的执行仍然发生在warp，It is not possible for threads in a warp to perform different instructions in the same cycle
    - **也就是说仍然无法一起执行A和B**
- ITS provides a “progress guarantee”: eventually, over a number of cycles, all individual program counters that the threads in a warp maintain will be visited
- they are free to stay diverged until the program finishes. The GPU will try to make threads reconverge at opportune times,
- 通过显示的同步指令保证所有线程执行相同的指令。synchronization command

![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231221153639.png)



`__syncwarp`
- 只对volta之后的架构有意义（支持ITS）
- 可以通过mask只同步一个子集线程。32bit integer, where each bit indicates whether or not a thread with the corresponding ID should participate in the synchronization.

`__syncthreads()`
- All active threads must reach the same instruction in the program

`this_grid().sync()` can busy-wait to synchronize entire kernel


正常来说，CUDA编程范式（programming paradigm）包含grid-block-thread三层。但是由于线程是按照warp调度的，因此正确利用warp的性质可以极大提高性能。

**Warp-level primitives** are instructions where threads in one warp exploit the fact that they run together to quickly share information


![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231221155633.png)

### reduce优化例子

1 reduce global
- N次read, store global memory
- read会经过cache，那么其它sm读取的时候会反复出现invalid cache的情况吗？
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231221160954.png)


2 reduce shared
- 使用shared快速的内存累加
- 将操作global memory降低为N/256
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231221161129.png)


前面的均是顺序规约，可以使用subliner的算法
3 
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231221162012.png)

4 
- 使用寄存器做最后32个数据的规约
    - In the first iteration, each thread in the warp will try to read the value of the thread with an ID that is 16 higher than its own.
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231221162702.png)


![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231221163110.png)

### stream

CUDA will assume that kernels depend on each other unless indicated otherwise.

stream
- stream之间没有依赖
- launch时通过第4个参数指定stream，默认为“null” stream
    - 调用CUDA runtime API时，传递stream参数。如cudaMemcpyAsync

![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231221185443.png)


### debug

nsight vscode plugin: Nsight Visual Studio Code Edition

- Overview reveals warps, active and valid masks of individual threads


cuda-memcheck suit
`cuda-memcheck –-tool <tool> <application>`
• memcheck: memory errors (access violations, leaks, API errors) 
• synccheck: misuse of synchronization (invalid masks, conditions) 
• racecheck: data races (read-after-write, write-after-read hazards) 
• initcheck: evaluation of uninitialized values (global memory only


- CUB/Thrust: additional primitives and functions similar to standard library 
    - Algorithms: e.g., prefix sum, scan, sort 
    • Data structures and containers: e.g., vectors 
- cuBLAS: basic linear algebra subprograms (BLAS) on top of CUDA 
- cuFFT: efficient implementation of discrete fourier transform on the GPU 
- cuSparse: algorithms and optimizations for working with sparse matrices 
- TensorRT: interface to learning and inference capabilities with tensor cores 
- CUTLASS: provides a range of templates for tensor core matrix computations


[Compiler Explorer (godbolt.org)](https://godbolt.org/)
- 支持cuda c++
- 支持PTX和SASS


## part 2 hardware实现

### 编译

host和device部分分开编译

host code
- takes care of loading matching GPU binary stored in .exe 
- translate kernel`<<<…>>>(…)` syntax into API call

“Fat Binary” can contain both 
- PTX for various compute capabilities 
    - allows the binary to target unknown architecture 
- precompiled machine code for specific GPU architectures 
    - optimal performance on certain known devices

PTX
- 生成机器码
    - ptxas
    - driver at runtime(JIT)

SASS(Shader Assembly?)
- nvdisasm.exe
- 没有文档

### 硬件设计

- GPU设计成最大化吞吐量
- 不等待长延迟操作，而是切换到其它任务

设计对比
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231221193700.png)

CPU：优化single flow速度
- larget cache
- complex control logic for out-of-order execution

GPU
- simple control
- many core
- large register
    - keep context resident on chip, enable rapidly switch
- rely on SIMD
    - Each control logic is associated with multiple arithmetic logic units (ALUs), allowing for the parallel execution of multiple control flows


two-level hierarchy that consists of **groups of groups of SIMD cores**.
- GPU
    - SM
        - warp scheduler
            - register file
            - a set of ALU: INT32, FP32, FP64, Tensor Core, LD/ST unit, SFU
        - shared memory
        - L1 cache


each warp scheduler is assigned a set of warps to execute
- The execution context of all threads that are part of each warp is kept locally in the register file associated with the warp scheduler responsible for the warp

Whenever it could issue a new instruction (e.g., **each clock cycle** unless all of the necessary resources are occupied), the warp scheduler will **pick the next instruction** from one of its warps that are ready to run (i.e., not waiting on the result of a previous operation) **and schedule it to the appropriate hardware unit.**
- 单发射？
- 看来时SIMD形式发射

warp切换隐藏延迟，warp scheduler从reay的warp中选择一个执行其指令（假设SIMD），遇到访存指令，如果下一条指令依赖访存指令数据，则将warp置于suspend queue。然后切换到另一个ready warp。因此只要一直能切换warp，访存延迟就可以被隐藏。

So far, we have simply assumed that all threads within a warp always need to run the same instruction next, allowing us to execute all threads of a warp in parallel in SIMD fashion

### warp实现

对于diverge的处理，属于猜测

**predication**
- 指令通过1bit prediction，来决定是否执行
- 程序一直执行下一条指令，有一些分支的EXIT指令在pred的作用下生效，避免执行其它分支的代码
- 缺点：
    - However, control flow is forced to pass over all instructions on both sides of every branch
        - 有些branch，没有线程执行。仍然要遍历所有path，导致significant overhead especially as branches are nested more and more deeply
- 对于更复杂的控制，比如函数调用，无法实现

先执行一部分，再执行另一部分，最后合并。
- 如何决定执行顺序呢？
    - 顺序执行，保证能遍历到所有分支
- 更复杂的情况？
    - 嵌套循环
    - 必须遍历完所有分支（即使没有线程执行）
    - **使用CSR栈**

![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231222151515.png)


**CSR**(Call, Return, Synchronization)
- warp调度器维护一个栈：mask(32bit)，合并点
- 遇到分支指令时，将一个分支的mask和合并点push到stack，然后执行另一个分支(~mask)，结束时sync，从stack中pop恢复

[CUDA and Application to Task-Based Programming (part 1) | Eurographics'2021 Tutorial (youtube.com)](https://www.youtube.com/watch?v=6kT7vVHCZIc)
- 2:17:30，解释CSR


ITS
- Instead of just scheduling warps as a whole, the warp scheduler can switch between active branches of warps. **While execution of divergent branches still has to be serialized, it can be interleaved**
    - 调度的单位从warp变为了branch
- 消除了intra-warp synchronization的限制
- 延迟隐藏：larger set of potential work to choose from

![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231221205727.png)


![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231221212607.png)

### memory层次


![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231222152005.png)

In CUDA, these hardware resources are exposed in the form of a number of memory spaces, each with different properties designed for different kinds of access patterns.
#### global

- 一般的数据存储
- 很慢：bandwidth: ≈ 300–700 GiB/s (GDDR5/6 vs HMB2)
    • non-cached coalesced access: 375 cycles 
    • L2 cached access: 190 cycles 
    • L1 caches access: 30 cycles
- cache设计：
    - 不是为了利用时间复用，而是smooth-out access patterns
        -  64 B L1\$, 37 B L2\$ per thread
    - 不要像CPU一样分块，而是使用shared memory
    - L1写直达，L2写回

- try to get coalescing per warp
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231222153146.png)


粒度实验
- 性能随着访问的cacheline数目增长而线性降低
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231222133927.png)


**vector load/store**

- 计算地址没办法延迟隐藏，因为访存操作对其结果有依赖

`__restrict`：This instructs the compiler to assume that the input and output will never point to overlapping memory regions.
- As a result, the compiler can potentially, e.g., fetch input data through faster **non-coherent caches**.

128bit宽（16B）


#### const

![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231222135535.png)

- cached
    - const cache，过去叫texture cache

- read only, uniform
    - e.g.: coefficients, used to pass kernel parameters
- broadcasting
    - all thread read same value
    - otherwise diverged, slowdown
- limited to 64 KiB

- As long as access happens uniformly, constant memory can provide very low access latencies.
- For divergent access patterns, **normal global memory** can typically provide better access latencies due to its different cache design (as long as the access hits the cache).
#### tex

optimized for 2D spatial access.
- optimal for neither row-wise nor column-wise access, it can handle both with similar efficiency.

prior to Volta 
• Textures tend to perform at least as good, sometimes better 
• put less stress on L2 cache 
• L1 cache free for other tasks

now 
• advanced Unified Cache (L1 + Tex) 
• Textures still perform best for spatially local access pattern 
• but can also be slower if access pattern and cache hits favor linear memory


#### shared memory


crossbar
- 32bit bank
- simultaneous to distinct bank
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231222142759.png)

shared memory
- 32 bank, consecutive 4-byte elements map to one bank
- Each memory bank can serve **4-byte loads and stores** from and to the addresses it’s responsible for.
- 同一时间不能有多个thread访问同一个bank
    - N-way bank conflict
    - 特殊情况，访问相同地址，可以广播
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231222142604.png)

32x32的matrix，对行列分别求和，使用32个线程并行
- 每列元素位于相同bank，因此行求和时，每个线程刚好访问相同bank，导致冲突
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231222143825.png)

trick
- 添加一个dummy colum后，每行每列元素均位于不同bank
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231222144217.png)


作用
- inter-thread communication：block内的一个共享变量
- **reduce global memory access -> manual cache**
    - 先把数据从global load到shared memory再处理
- adjust global memory access pattern
- indexed access 
    - 普通变量数组，会spill到local memory中
- combine costly operations

![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231222144559.png)


## part 3

### managed memory

static分配: `device`
dynamic分配(cpu代码执行分配)：cudaMalloc

movement
- cudaMemcpy, cudaMemcpyToSymbol, cudaMemcpyFromSymbol

managed mem
- static: `__managed__`
- dynamic: `cudaMallocManaged`

在CC(compute capac)6.0之前，data migration是粗粒度的，不支持CPU和GPU concurrent access。
6.0之后，引入细粒度的page fault system，页为粒度，性能大幅提高
-  concurrent access仍然不能保证。

### ITS

legacy
- 所有线程位于同一个PC。需要执行完一个分支后再执行另一个（因为只有一个PC，线程无法单独记录自己的位置）
- 执行一个分支时，将另一个分支压入栈中即可（dfs？）该分支执行结束时通过sync语句弹栈，回到合并点。

its
- 可以用于实现锁
- 只保证resident warp是可以执行完的
    - thread will wait forever if their progress depends on non-redident warp
    - 一个SM能同时执行的warp是有限的，如果warp太多，会放在buffer里，仍有可能导致死锁？

- 需要显示同步warp
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231226161727.png)

later warp primitve
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231226164604.png)
reduce(>8.0)
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231226164654.png)

### tensor core

- cublas以后默认会尽可能使用tensor core，除非定义CUDA_PREDANTIC_MATH
- baseline是使用tiling, shared memroy，可以看到cublas非常重要。
    - 但是分析模型如何分析闭源的SASS可执行文件呢？
![](https://raw.githubusercontent.com/TheRainstorm/.image-bed/main/20231226163426.png)

## summary

- [03-CUDA-Fundamental-Optimization-Part-1.pdf (ornl.gov)](https://www.olcf.ornl.gov/wp-content/uploads/2019/12/03-CUDA-Fundamental-Optimization-Part-1.pdf)
